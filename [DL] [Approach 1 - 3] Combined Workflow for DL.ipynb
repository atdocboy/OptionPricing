{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import math\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler\n",
    "from sklearn.utils import class_weight\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "import joblib\n",
    "# import py_vollib.black_scholes.implied_volatility\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import py_vollib.black_scholes.implied_volatility as vb\n",
    "import statistics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path settings\n",
    "\n",
    "\n",
    "path_var = 1 \n",
    "\n",
    "if (path_var == 1):\n",
    "    filepath = '/home/docboy/Desktop/Option_Pricing/datasets/'\n",
    "    modelpath = '/home/docboy/Desktop/Option_Pricing/pickles/'\n",
    "\n",
    "\n",
    "else:\n",
    "    filepath = '/home/sharan/Desktop/Option_Pricing/datasets/'\n",
    "    mdoelpath = '/home/sharan/Desktop/Option_Pricing/pickles/'\n",
    "\n",
    "\n",
    "\n",
    "nfty = filepath + 'NIFTY50_feature_set_COV_complete.csv'\n",
    "nfty_ecdf = filepath + 'NIFTY50_feature_set_ECDF_complete.csv'\n",
    "    \n",
    "nfty_19 = filepath + 'NIFTY50_2019_feature_set_COV_complete.csv'\n",
    "nfty_ecdf_19 = filepath + 'NIFTY50_2019_feature_set_ECDF_complete.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach var set to :: 3\n"
     ]
    }
   ],
   "source": [
    "# Set the automated variables + Plus drop all useless columns approach specific\n",
    "\n",
    "\n",
    "###########################\n",
    "###############################################\n",
    "\n",
    "approach = 3                      #------------------>>> # Change value as needed\n",
    "\n",
    "###############################################\n",
    "###########################\n",
    "\n",
    "\n",
    "if (approach == 1):\n",
    "    feats = pd.read_csv(nfty_ecdf)\n",
    "    print ('Approach var set to ECDF')\n",
    "    \n",
    "    dropNames = ['date', 'percentage','strike','spot','opPrice']\n",
    "    \n",
    "    test_feats = pd.read_csv(nfty_ecdf_19)\n",
    "    \n",
    "else:\n",
    "    feats = pd.read_csv(nfty)\n",
    "    print ('Approach var set to :: '+ str(approach))\n",
    "    \n",
    "    test_feats = pd.read_csv(nfty_19)\n",
    "    \n",
    "    if (approach == 2):\n",
    "        dropNames= ['date', 'percentage','strike','spot','opPrice','change_oi','ts_atr','prevOp',\n",
    "                    'avg_atmError']\n",
    "        \n",
    "    else:\n",
    "        dropNames= ['date', 'percentage','strike','spot','opPrice','ts_atr','change_oi']\n",
    "        \n",
    "###################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X=feats.iloc[:,:-1]\n",
    "Y=feats.loc[:,'Target']\n",
    "X = X.drop(dropNames, axis = 1)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G = test_feats.iloc[:,:-1]\n",
    "H = test_feats.loc[:,'Target']\n",
    "G = G.drop(dropNames, axis = 1)  \n",
    "\n",
    "\n",
    "# Port part handled at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['close_mean', 'close_sd', 'open_mean', 'open_sd', 'high_mean',\n",
       "       'high_sd', 'low_mean', 'low_sd', 'close_lr_open_lr_cov',\n",
       "       'close_lr_high_lr_cov', 'close_lr_low_lr_cov', 'open_lr_high_lr_cov',\n",
       "       'open_lr_low_lr_cov', 'high_lr_low_lr_cov', 'avg_atmError', 'prevOp',\n",
       "       'ttm', 'atmError', 'yield03'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the scaling process\n",
    "\n",
    "featNames = list(X)  \n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns = featNames)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "G = scaler.transform(G)\n",
    "G = pd.DataFrame(G, columns = featNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Save the scalar file\n",
    "# joblib.dump(scaler, modelpath + 'DL_SCALER_NIFTY_0.02_Expt' + str(approach) + '.model')\n",
    "# print(\"Scaler saved to path ::\", modelpath + 'DL_SCALER_NIFTY_0.02_Expt' + str(approach) + '.model' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = OneHotEncoder(sparse=False)\n",
    "Y = Y.values\n",
    "Y = Y.reshape(len(Y), 1)\n",
    "Y = onehot.fit_transform(Y)\n",
    "\n",
    "\n",
    "onehot = OneHotEncoder(sparse=False)\n",
    "H = H.values\n",
    "H = H.reshape(len(H), 1)\n",
    "H = onehot.fit_transform(H)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X & Y_Dim of Train is (10837, 19) (10837, 50)\n",
      "X & Y_Dim of Test is (7667, 19) (7667, 50)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "feats['date'] = pd.to_datetime(feats['date'])\n",
    "test=feats[(feats['date']>=datetime.datetime(2017,10,1))]\n",
    "split_val = test.index[0]\n",
    "\n",
    "X_train = X.iloc[0:split_val, :]\n",
    "# X_test = X.iloc[split_val:, :]\n",
    "Y_train = Y[:split_val]\n",
    "# # Y_test = Y[split_val:]\n",
    "# print ('X & Y_Dim of Train is', X_train.shape, Y_train.shape)\n",
    "# print ('X & Y_Dim of Test is', X_test.shape, Y_test.shape)\n",
    "\n",
    "\n",
    "# X_train = X\n",
    "\n",
    "\n",
    "X_test = G\n",
    "\n",
    "\n",
    "# Y_train = Y\n",
    "Y_test = H\n",
    "\n",
    "\n",
    "\n",
    "print ('X & Y_Dim of Train is', X_train.shape, Y_train.shape)\n",
    "print ('X & Y_Dim of Test is', X_test.shape, Y_test.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               2560      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                3250      \n",
      "=================================================================\n",
      "Total params: 14,066\n",
      "Trainable params: 14,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10837 samples, validate on 7667 samples\n",
      "Epoch 1/40\n",
      "10837/10837 [==============================] - 2s 207us/step - loss: 3.7990 - accuracy: 0.0576 - val_loss: 3.5659 - val_accuracy: 0.1071\n",
      "Epoch 2/40\n",
      "10837/10837 [==============================] - 2s 205us/step - loss: 3.3929 - accuracy: 0.0737 - val_loss: 3.1411 - val_accuracy: 0.1183\n",
      "Epoch 3/40\n",
      "10837/10837 [==============================] - 2s 145us/step - loss: 3.0595 - accuracy: 0.0917 - val_loss: 2.9970 - val_accuracy: 0.1274\n",
      "Epoch 4/40\n",
      "10837/10837 [==============================] - 2s 163us/step - loss: 2.8552 - accuracy: 0.1269 - val_loss: 2.9757 - val_accuracy: 0.1354\n",
      "Epoch 5/40\n",
      "10837/10837 [==============================] - 2s 144us/step - loss: 2.7121 - accuracy: 0.1505 - val_loss: 2.9398 - val_accuracy: 0.1446\n",
      "Epoch 6/40\n",
      "10837/10837 [==============================] - 2s 156us/step - loss: 2.6003 - accuracy: 0.1685 - val_loss: 2.8948 - val_accuracy: 0.1527\n",
      "Epoch 7/40\n",
      "10837/10837 [==============================] - 2s 143us/step - loss: 2.5051 - accuracy: 0.1827 - val_loss: 2.8712 - val_accuracy: 0.1593\n",
      "Epoch 8/40\n",
      "10837/10837 [==============================] - 2s 141us/step - loss: 2.4207 - accuracy: 0.2042 - val_loss: 2.8902 - val_accuracy: 0.1609\n",
      "Epoch 9/40\n",
      "10837/10837 [==============================] - 1s 127us/step - loss: 2.3468 - accuracy: 0.2185 - val_loss: 2.9116 - val_accuracy: 0.1681\n",
      "Epoch 10/40\n",
      "10837/10837 [==============================] - 2s 139us/step - loss: 2.2816 - accuracy: 0.2286 - val_loss: 2.9204 - val_accuracy: 0.1775\n",
      "Epoch 11/40\n",
      "10837/10837 [==============================] - 1s 119us/step - loss: 2.2217 - accuracy: 0.2458 - val_loss: 2.9987 - val_accuracy: 0.1890\n",
      "Epoch 12/40\n",
      "10837/10837 [==============================] - 1s 117us/step - loss: 2.1695 - accuracy: 0.2584 - val_loss: 3.0426 - val_accuracy: 0.1929\n",
      "Epoch 13/40\n",
      "10837/10837 [==============================] - 2s 155us/step - loss: 2.1238 - accuracy: 0.2685 - val_loss: 3.0440 - val_accuracy: 0.1938\n",
      "Epoch 14/40\n",
      "10837/10837 [==============================] - 2s 139us/step - loss: 2.0815 - accuracy: 0.2795 - val_loss: 3.1437 - val_accuracy: 0.2028\n",
      "Epoch 15/40\n",
      "10837/10837 [==============================] - 2s 172us/step - loss: 2.0445 - accuracy: 0.2896 - val_loss: 3.1410 - val_accuracy: 0.2040\n",
      "Epoch 16/40\n",
      "10837/10837 [==============================] - 2s 175us/step - loss: 2.0092 - accuracy: 0.2999 - val_loss: 3.2970 - val_accuracy: 0.2044\n",
      "Epoch 17/40\n",
      "10837/10837 [==============================] - 2s 149us/step - loss: 1.9815 - accuracy: 0.3035 - val_loss: 3.2247 - val_accuracy: 0.2131\n",
      "Epoch 18/40\n",
      "10837/10837 [==============================] - 2s 149us/step - loss: 1.9549 - accuracy: 0.3117 - val_loss: 3.3415 - val_accuracy: 0.2142\n",
      "Epoch 19/40\n",
      "10837/10837 [==============================] - 2s 166us/step - loss: 1.9307 - accuracy: 0.3171 - val_loss: 3.2120 - val_accuracy: 0.2155\n",
      "Epoch 20/40\n",
      "10837/10837 [==============================] - 1s 118us/step - loss: 1.9086 - accuracy: 0.3198 - val_loss: 3.3806 - val_accuracy: 0.2169\n",
      "Epoch 21/40\n",
      "10837/10837 [==============================] - 1s 135us/step - loss: 1.8881 - accuracy: 0.3225 - val_loss: 3.3978 - val_accuracy: 0.2174\n",
      "Epoch 22/40\n",
      "10837/10837 [==============================] - 2s 142us/step - loss: 1.8706 - accuracy: 0.3309 - val_loss: 3.4901 - val_accuracy: 0.2211\n",
      "Epoch 23/40\n",
      "10837/10837 [==============================] - 2s 169us/step - loss: 1.8533 - accuracy: 0.3354 - val_loss: 3.4168 - val_accuracy: 0.2224\n",
      "Epoch 24/40\n",
      "10837/10837 [==============================] - 2s 175us/step - loss: 1.8365 - accuracy: 0.3341 - val_loss: 3.4935 - val_accuracy: 0.2199\n",
      "Epoch 25/40\n",
      "10837/10837 [==============================] - 2s 177us/step - loss: 1.8239 - accuracy: 0.3375 - val_loss: 3.5310 - val_accuracy: 0.2262\n",
      "Epoch 26/40\n",
      "10837/10837 [==============================] - 2s 171us/step - loss: 1.8117 - accuracy: 0.3390 - val_loss: 3.5588 - val_accuracy: 0.2253\n",
      "Epoch 27/40\n",
      "10837/10837 [==============================] - 2s 185us/step - loss: 1.7973 - accuracy: 0.3419 - val_loss: 3.5270 - val_accuracy: 0.2275\n",
      "Epoch 28/40\n",
      "10837/10837 [==============================] - 2s 168us/step - loss: 1.7857 - accuracy: 0.3420 - val_loss: 3.6136 - val_accuracy: 0.2297\n",
      "Epoch 29/40\n",
      "  736/10837 [=>............................] - ETA: 1s - loss: 1.7647 - accuracy: 0.3601"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1ed506b38263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#   class_weight=class_weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#   shuffle=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   validation_data=(X_test, Y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt=optimizers.Adam(lr=0.00018);\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(units=128, activation='relu',input_dim=X_train.shape[1]))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "# model.add(keras.layers.Dense(units=128, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(units=64, activation='relu'))\n",
    "# model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(units=50, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy'],)\n",
    "history=model.fit(X_train, Y_train,\n",
    "  batch_size=32,\n",
    "  epochs=40,\n",
    "  verbose=1,\n",
    "#   callbacks=[EarlyStopping(monitor='val_loss', patience=1,min_delta=-0.01)],\n",
    "#   class_weight=class_weights,\n",
    "#   shuffle=True,                \n",
    "  validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(path_var ==1):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "elif(path_var==2):\n",
    "\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test accuracy:', score[1]*100)\n",
    "\n",
    "Y_pred=model.predict(X_test)\n",
    "Y_pred=np.argmax(Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OGY_test=onehot.inverse_transform(Y_test)\n",
    "# OGY_test=sum(OGY_test, [])\n",
    "OGY_test=OGY_test.ravel()\n",
    "# print(OGY_test)\n",
    "\n",
    "# # Check if Binwidth value exists\n",
    "maxVal=5\n",
    "minVal=0\n",
    "\n",
    "width = maxVal - minVal\n",
    "\n",
    "num_bins = 50\n",
    "\n",
    "\n",
    "bin_width = width/num_bins\n",
    "# print(bin_width)\n",
    "\n",
    "# Normalized Error Metric (NEM)\n",
    "diff = []\n",
    "dist_sum = 0\n",
    "for i in range(len(Y_pred)):\n",
    "    dist = abs(Y_pred[i] - OGY_test[i])\n",
    "    d = Y_pred[i] - OGY_test[i]\n",
    "    diff.append(d)\n",
    "    dist_sum = dist_sum + dist\n",
    "    \n",
    "print((dist_sum*bin_width)/len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u = {'C_i' : OGY_test, 'P_i' : Y_pred, 'C_i-P_i': diff}\n",
    "# g = pd.DataFrame(u)\n",
    "# g.to_csv(filepath + 'ANN_Cipi.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_int(lst):\n",
    "    df = pd.DataFrame(lst)\n",
    "    df = df.sort_values(by = 0).reset_index(drop=True)\n",
    "    cp1 = df[np.logical_and(df[0] >= -2,df[0] <= 2)]\n",
    "    return (len(cp1)/len(df))*100\n",
    "   \n",
    "# lst = Y_test - k\n",
    "# c_int(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval:63.49289161340811\n"
     ]
    }
   ],
   "source": [
    "CP=[]\n",
    "for i in range(len(Y_pred)):\n",
    "    dist = OGY_test[i] - Y_pred[i]\n",
    "    CP.append(dist)\n",
    "CP=np.array(CP,dtype=float)\n",
    "\n",
    "print(\"Confidence Interval:{}\".format(c_int(CP)))\n",
    "# np.savetxt(filepath+\"CPNIFTY.csv\",CP, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to path :: /home/sharan/Desktop/OptionPricing-master/pickles/DL_NIFTY_0.02_Expt1.model\n"
     ]
    }
   ],
   "source": [
    "# # Saving the model\n",
    "# joblib.dump(model, modelpath + 'DL_NIFTY_0.02_Expt' + str(approach) + '.model')\n",
    "# print(\"Model saved to path ::\", modelpath + 'DL_NIFTY_0.02_Expt' + str(approach) + '.model' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       197\n",
      "           1       0.38      0.41      0.39       160\n",
      "           2       0.26      0.34      0.29       132\n",
      "           3       0.19      0.13      0.15       118\n",
      "           4       0.12      0.12      0.12       107\n",
      "           5       0.08      0.03      0.04       104\n",
      "           6       0.16      0.27      0.20        93\n",
      "           7       0.17      0.24      0.20        90\n",
      "           8       0.22      0.09      0.13        78\n",
      "           9       0.21      0.07      0.11        84\n",
      "          10       0.16      0.40      0.23        70\n",
      "          11       0.50      0.01      0.03        68\n",
      "          12       0.20      0.29      0.24        58\n",
      "          13       0.21      0.27      0.24        63\n",
      "          14       0.18      0.05      0.08        62\n",
      "          15       0.12      0.12      0.12        50\n",
      "          16       0.18      0.25      0.21        55\n",
      "          17       0.23      0.17      0.20        53\n",
      "          18       0.12      0.06      0.08        52\n",
      "          19       0.17      0.36      0.23        50\n",
      "          20       0.12      0.07      0.09        45\n",
      "          21       0.58      0.12      0.20        59\n",
      "          22       0.23      0.15      0.18        47\n",
      "          23       0.18      0.55      0.27        42\n",
      "          24       0.00      0.00      0.00        40\n",
      "          25       0.08      0.02      0.04        41\n",
      "          26       0.23      0.45      0.30        49\n",
      "          27       0.09      0.12      0.10        42\n",
      "          28       0.00      0.00      0.00        30\n",
      "          29       0.00      0.00      0.00        34\n",
      "          30       0.11      0.36      0.17        42\n",
      "          31       0.14      0.13      0.14        39\n",
      "          32       0.00      0.00      0.00        35\n",
      "          33       0.17      0.04      0.06        25\n",
      "          34       0.00      0.00      0.00        38\n",
      "          35       0.07      0.03      0.04        34\n",
      "          36       0.10      0.17      0.13        30\n",
      "          37       0.14      0.41      0.21        32\n",
      "          38       0.00      0.00      0.00        31\n",
      "          39       0.12      0.09      0.10        23\n",
      "          40       0.21      0.11      0.14        28\n",
      "          41       0.11      0.07      0.09        28\n",
      "          42       0.00      0.00      0.00        25\n",
      "          43       0.14      0.45      0.21        29\n",
      "          44       0.00      0.00      0.00        14\n",
      "          45       0.00      0.00      0.00        18\n",
      "          46       0.00      0.00      0.00        10\n",
      "          47       0.00      0.00      0.00         8\n",
      "          48       0.00      0.00      0.00         7\n",
      "          49       0.23      0.90      0.37        10\n",
      "\n",
      "    accuracy                           0.23      2679\n",
      "   macro avg       0.15      0.17      0.14      2679\n",
      "weighted avg       0.22      0.23      0.20      2679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix,classification_report\n",
    "# print(classification_report(OGY_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size=6792\n",
      "b_X & b_Y_Dim of Test is (6792, 22) (6792, 50)\n"
     ]
    }
   ],
   "source": [
    "# b_feats['date'] = pd.to_datetime(b_feats['date'])\n",
    "# b_test=b_feats[(b_feats['date']>=datetime.date(2017,10,1))]\n",
    "# ind = b_test.index[0]\n",
    "\n",
    "# b_X = b_feats.iloc[ind:,:-1]\n",
    "# b_Y = b_feats.loc[ind:,'Target']\n",
    "# b_X = b_X.drop(dropNames, axis = 1)\n",
    "\n",
    "# featNames = list(b_X)\n",
    "# b_X = scaler.transform(b_X)\n",
    "# b_X = pd.DataFrame(b_X, columns = featNames)\n",
    "\n",
    "# print(\"Test Size={}\".format(len(b_Y)))\n",
    "\n",
    "# # Generate Class weights\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced',np.unique(b_Y),b_Y)\n",
    "\n",
    "\n",
    "# # Port the Onehot tranform\n",
    "# b_Y = b_Y.values\n",
    "# b_Y = b_Y.reshape(len(b_Y), 1)\n",
    "# b_Y = onehot.transform(b_Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Split the train and test sets without random optimizations\n",
    "\n",
    "\n",
    "# b_X_test = b_X.iloc[:, :]\n",
    "# b_Y_test = b_Y[:]\n",
    "# print ('b_X & b_Y_Dim of Test is', b_X_test.shape, b_Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 24.263839811542994\n"
     ]
    }
   ],
   "source": [
    "# score = model.evaluate(b_X_test, b_Y_test, verbose=0)\n",
    "# print('Test accuracy:', score[1]*100)\n",
    "\n",
    "# b_Y_pred=model.predict(b_X_test)\n",
    "# b_Y_pred=np.argmax(b_Y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18938457008244997\n"
     ]
    }
   ],
   "source": [
    "# b_OGY_test = onehot.inverse_transform(b_Y_test)\n",
    "# b_OGY_test = b_OGY_test.ravel()\n",
    "\n",
    "# # Check if Binwidth value exists\n",
    "# maxVal=5\n",
    "# minVal=0\n",
    "\n",
    "# width = maxVal - minVal\n",
    "\n",
    "# num_bins = 50\n",
    "\n",
    "\n",
    "# bin_width = width/num_bins\n",
    "# # print(bin_width)\n",
    "\n",
    "# # Normalized Error Metric (NEM)\n",
    "\n",
    "# dist_sum = 0\n",
    "# for i in range(len(b_Y_pred)):\n",
    "#     dist = abs(b_Y_pred[i] - b_OGY_test[i])\n",
    "#     dist_sum = dist_sum + dist\n",
    "    \n",
    "# print((dist_sum*bin_width)/len(b_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval:71.52532391048292\n"
     ]
    }
   ],
   "source": [
    "# CP=[]\n",
    "# for i in range(len(b_Y_pred)):\n",
    "#     dist = b_OGY_test[i] - b_Y_pred[i]    \n",
    "#     CP.append(dist)\n",
    "# CP=np.array(CP,dtype=float)\n",
    "\n",
    "# print(\"Confidence Interval:{}\".format(c_int(CP)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
